{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "https://wikidocs.net/30708\n",
    "\n",
    "1.문서에 사용할 단어의 개수 N 설정  \n",
    "2.문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정  \n",
    "3.문서에 사용할 각 단어를 설정    \n",
    "3-1.토픽 분포에서 토픽T를 확률적으로 선택    \n",
    "3-2.선택한 토픽T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어 선택\n",
    "\n",
    "1.토픽 개수 k설정  \n",
    "2.모든 단어를 k개 중 하나의 토픽에 할당  \n",
    "3.모든 문서의 모든 단어에 대해 아래 사항 반복  \n",
    "3-1.어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당 되어져 있지만,  \n",
    "다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정.  \n",
    "이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당.   \n",
    "  \n",
    "$P{(topic\\,t | document\\,d)}$ : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율  \n",
    "$P{(word\\,w|topic\\,t)}$ : 각 토픽들 t에서 해당 단어 w의 분포\n",
    "\n",
    "이를 반복하면, 모든 할당이 완료되어 수렴.  \n",
    "두가지 기준의 예시\n",
    "\n",
    "![LDA1](./lda1.png)  \n",
    "두 개의 문서 doc1과 doc2  \n",
    "doc1의 세번째 단어 apple의 토픽 결정\n",
    "\n",
    "![LDA2](./lda3.png)  \n",
    "첫번째로 사용하는 기준은 doc1의 단어들이 어떤 토픽에 해당하는지 확인.  \n",
    "doc1의 모든 단어들은 토픽A와 토픽B에 50:50의 비율로 할당.  \n",
    "apple은 토픽A 또는 토픽B 둘 중 어디에도 속할 가능성이 존재\n",
    "\n",
    "![LDA3](./lda2.png)  \n",
    "두번째 기준은 단어 apple이 전체 문서에서 어떤 토픽에 할당되어져 있는지 확인.  \n",
    "이 기준에 따르면 단어 apple은 토픽 B에 할당될 가능성이 높음.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ko.wikipedia.org/wiki/%EC%9E%A0%EC%9E%AC_%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%ED%95%A0%EB%8B%B9\n",
    "\n",
    "M개의 문서가 주어져 있고, 모든 문서는 각각 k개의 주제 중 하나에 속할 때,  \n",
    "- 단어는 이산 자료의 기본 단위로 Vocabulary의 인덱스로 나타낼 수 있다.  \n",
    "단어집의 크기를 V라고 하면, 각각의 단어는 인덱스 $v \\in \\{1,...,V\\}$로 대응된다.  \n",
    "단어 벡터 $w$는 $V-$벡터로 표기하며 $w^v = 1, w^u=0, u\\neq v$를 만족한다.  \n",
    "- 문서는 N개의 단어의 연속으로 나타내며, w $= (w_1,w_2,...,w_N)$ 으로 표기한다.  \n",
    "- 전집은 M개의 집합으로 나타내며, $D = \\{$w$_1,$w$_2,...,$w$_M\\}$으로 표기한다.\n",
    "\n",
    "  \n",
    "LDA는 각각의 문서 w $\\in D$에 대해 다음과 같은 생성 과정을 가정한다.  \n",
    "1. $N$ ~ $Poisson(\\xi)$ 을 선택한다.  \n",
    "2. $\\theta$ ~ Dir($\\alpha$) 를 선택한다.  \n",
    "3. 문서 내의 단어 $w_n \\in $w 에 대해서  \n",
    "(a) $z_n$ ~ Multinomial($\\theta$) 를 선택한다.  \n",
    "(b) $z_n$이 주어졌을 때 $w_n$는 $P(w_n|z_n,\\beta)$ 로부터 선택한다.  \n",
    "생성 과정에서 각각의 변수는 다음과 같은 의미를 가진다.  \n",
    "\n",
    "$\\alpha$는 $k$차원 디리클레 분포의 매개변수이다.  \n",
    "$\\theta$는 $k$차원 벡터이며, $\\theta^i$는 문서가 $i$번째 주제에 속할 확률 분포를 나타낸다.  \n",
    "$z_n$는 $k$차원 벡터이며, $z_n^i$는 단어 $w_n$이 $i$번째 주제에 속할 확률 분포를 나타낸다.  \n",
    "$\\beta$는 $k$ X $V$ 크기의 행렬 매개변수로, $\\beta_{ij}$는 $i$번째 주제가 Vocabulary의 $j$번째 단어를 생성할 확률을 나타낸다.  \n",
    "\n",
    "여기에서 $w_n$는 실제 문서를 통해 주어지며, 다른 변수는 관측할 수 없는 잠재 변수이다.  \n",
    "이 모형은 다음과 같이 해석될 수 있다. 각 문서에 대해 $k$개의 주제에 대한 가중치 $\\theta$가 존재한다.  \n",
    "문서 내의 각 단어 $w_n$은 $k$개의 주제에 대한 가중치 $z_n$을 가지는데, $z_n$은 $\\theta$에 의한 다항 분포로 선택된다.  \n",
    "마지막으로 실제 단어 $w_n$이 $z_n$에 기반하여 선택된다.  \n",
    "잠재 변수 $\\alpha, \\beta$가 주어졌을 때, $\\theta, $z $=\\{z_1,...,z_N\\},$w에 대한 결합 분포는 다음과 같이 구해진다.  \n",
    "### $$p(\\theta,z,w|\\alpha,\\beta) = p(\\theta|\\alpha)\\prod_{n=1}^N p(z_n|\\theta)p(w_n|z_n,\\beta),$$  \n",
    "여기서 $z_n$과 $\\theta$를 모두 더하여 문서의 주변 분포 (marginal distribution)를 구할 수 있다.  \n",
    "이 때 디 피네치의 정리 (de Finetti's theorem)에 의해 단어가 문서 안에서 교환성을 가지는 것을 확인할 수 있다.  \n",
    "### $$p(w|\\alpha,\\beta) = \\int  p(\\theta|\\alpha)  (\\prod_{n=1}^N \\sum_{z_n} p(z_n|\\theta)p(w_n|z_n,\\beta)) d\\theta$$  \n",
    "마지막으로 각각의 문서에 대한 주변 분포를 모두 곱하여 Vocabulary의 확률을 구할 수 있다.  \n",
    "### $$p(D|\\alpha,\\beta) = \\prod_{d=1}^M \\int  p(\\theta_d|\\alpha)  (\\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d)p(w_{dn}|z_{dn},\\beta)) d\\theta_d$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">M개의 문서가 주어져 있고, 모든 문서는 각각 k개의 주제 중 하나에 속할 때,  \n",
    "- 단어는 이산 자료의 기본 단위로 Vocabulary의 인덱스로 나타낼 수 있다.  \n",
    "Vocabulary의 크기를 V라고 하면, 각각의 단어는 인덱스 $v \\in \\{1,...,V\\}$로 대응된다.  \n",
    "단어 벡터 $w$는 $V-$벡터로 표기하며 $w^v = 1, w^u=0, u\\neq v$를 만족한다.  \n",
    "- 문서는 N개의 단어의 연속으로 나타내며, w $= (w_1,w_2,...,w_N)$ 으로 표기한다.  \n",
    "- 전집은 M개의 집합으로 나타내며, $D = \\{$w$_1,$w$_2,...,$w$_M\\}$으로 표기한다.  \n",
    "\n",
    "> example :\n",
    "[I have an apple. You have a banana.]\n",
    "[I am a boy. You are a girl.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([i for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sen = [\"I have an apple\",\n",
    "    \"You have a banana\",\n",
    "    \"I am a boy\",\n",
    "    \"You are a girl\"]\n",
    "\n",
    "sentences = [s.split() for s in sen]\n",
    "\n",
    "words = []\n",
    "w2i = {}\n",
    "i2w = {}\n",
    "\n",
    "for s in sentences:\n",
    "    for w in s:\n",
    "        if w in words:continue\n",
    "        words.append(w)\n",
    "        \n",
    "w2i = {words[i]:i for i in range(len(words))}\n",
    "i2w = {i:words[i] for i in range(len(words))}\n",
    "\n",
    "k = 4\n",
    "topics = [[random.choice([r for r in range(k)]) for i in range(len(sen[j].split()))] for j in range(len(sen))]\n",
    "\n",
    "d_topic = [[topics[i].count(j) for j in range(k)] for i in range(len(sentences))]\n",
    "t_word = [[0 for i in range(len(words))] for j in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences[i])):\n",
    "        t_word[topics[i][j]][w2i[sentences[i][j]]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3, 3, 2], [0, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1],\n",
       " [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 1, 2], [1, 2, 1, 0], [3, 1, 0, 0], [0, 2, 1, 1]]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 3, 2], [0, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 3, 2], [0, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 2], [0, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [0, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 2, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 1], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 1, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 3, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 2, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 1]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 2, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n",
      "[[0, 0, 0, 0], [1, 0, 1, 0], [0, 0, 0, 0], [1, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            d_topic[i][topics[i][j]]-=1\n",
    "            t_word[topics[i][j]][w2i[sentences[i][j]]]-=1\n",
    "            res = [d_topic[i][m]/(sum(d_topic[i])+1e-10) * t_word[m][w2i[sentences[i][j]]]/(sum(t_word[m])+1e-10) for m in range(k)]\n",
    "            res = res.index(max(res))\n",
    "            d_topic[i][res]+=1\n",
    "            t_word[res][w2i[sentences[i][j]]]+=1\n",
    "            topics[i][j] = res\n",
    "            print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'have', 'an', 'apple'],\n",
       " ['You', 'have', 'a', 'banana'],\n",
       " ['I', 'am', 'a', 'boy'],\n",
       " ['You', 'are', 'a', 'girl']]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topics)):\n",
    "    for j in range(len(topics[i])):\n",
    "        if topics[i][j]==2:print(sentences[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">LDA는 각각의 문서 w $\\in D$에 대해 다음과 같은 생성 과정을 가정한다.  \n",
    "1. $N$ ~ $Poisson(\\xi)$ 을 선택한다.  \n",
    "2. $\\theta$ ~ Dir($\\alpha$) 를 선택한다.  \n",
    "3. 문서 내의 단어 $w_n \\in $w 에 대해서  \n",
    "(a) $z_n$ ~ Multinomial($\\theta$) 를 선택한다.  \n",
    "(b) $z_n$이 주어졌을 때 $w_n$는 $P(w_n|z_n,\\beta)$ 로부터 선택한다.  \n",
    "\n",
    ">생성 과정에서 각각의 변수는 다음과 같은 의미를 가진다.  \n",
    "$\\alpha$는 $k$차원 디리클레 분포의 매개변수이다.  \n",
    "$\\theta$는 $k$차원 벡터이며, $\\theta^i$는 문서가 $i$번째 주제에 속할 확률 분포를 나타낸다.  \n",
    "$z_n$는 $k$차원 벡터이며, $z_n^i$는 단어 $w_n$이 $i$번째 주제에 속할 확률 분포를 나타낸다.  \n",
    "$\\beta$는 $k$ X $V$ 크기의 행렬 매개변수로, $\\beta_{ij}$는 $i$번째 주제가 Vocabulary의 $j$번째 단어를 생성할 확률을 나타낸다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 푸아송분포 (poisson distribution)\n",
    "\n",
    "정해진 시간 안에 어떤 사건이 일어날 횟수이 대한 기댓값을 $\\lambda$라고 했을 때, 그 사건이 $n$회 일어날 확률\n",
    "\n",
    "### $$f(n; \\lambda) = {\\lambda^n e^{-\\lambda}\\over n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디리클레분포 (Dirichlet distribution)\n",
    "\n",
    "$k$차원의 실수 벡터 중 벡터의 요소가 양수이며, 모든 요소를 더한 값이 1인 경우에 대해 확률값이 정의되는 분포.  \n",
    "\n",
    "### $$f(x_1,...,x_k;\\alpha_1,...,\\alpha_k) = {1\\over B(\\alpha)} \\prod_{i=1}^k x_i^{\\alpha_i -1}$$\n",
    "\n",
    "### $$B(\\alpha) = {\\prod_{i=1}^k \\Gamma(\\alpha_i) \\over \\Gamma(\\sum_{i=1}^k \\alpha_i)} $$\n",
    "\n",
    "### $$\\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} dt (Re~z > 0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$p(z_i=j|z_{-i},w)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
